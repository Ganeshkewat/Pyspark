{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1a85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH =\"C:\\\\Users\\\\GANESH\\\\Desktop\\\\pyspark\\\\dataset2\\\\\"\n",
    "FILE = \"marketing_sample_for_careerbuilder_usa-careerbuilder_job_listing__20200401_20200630__30k_data.ldjson\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66a6c0",
   "metadata": {},
   "source": [
    "#### Dendencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession as session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas\n",
    "pandas.set_option(\"display.max.column\",None)\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d47799",
   "metadata": {},
   "source": [
    "#### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa10f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = session.builder.appName(\"job\").getOrCreate()\n",
    "spark_df=spark.read.json(PATH+FILE,primitivesAsString = 'true')\n",
    "print(\"Spark Session Created Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d790ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b004e9",
   "metadata": {},
   "source": [
    "#### Drop duplicates and Unused Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2df510",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df =spark_df.drop('inferred_city','inferred_iso3_lang_code','inferred_salary_time_unit',\\\n",
    "                        'inferred_iso2_lang_code','inferred_state','last_expiry_check_date',\\\n",
    "                        'postdate_yyyymmdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d91ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropnul(spark_df):\n",
    "    null = spark_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in spark_df.columns]).\\\n",
    "          collect()[0].asDict()\n",
    "    col_to_drop = [key for key, value in null.items() if value > 0.7]\n",
    "    spark_df = spark_df.drop(*col_to_drop)\n",
    "    return spark_df\n",
    "dropnul(spark_df)\n",
    "new_spark_df = spark_df.transform(dropnul)\n",
    "new_spark_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbcdb25",
   "metadata": {},
   "source": [
    "calculate number of jobs posted on daily basis, per each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e227b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs=spark_df.groupBy([\"city\"]).agg(\n",
    "    count(\"post_date\").alias(\"total_jobs_post\"))\n",
    "jobs = jobs.sort(desc(\"total_jobs_post\")) \n",
    "\n",
    "# Convert to Pandas DataFrame and Save to Local\n",
    "josbs_pd_df =jobs.toPandas()\n",
    "josbs_pd_df.to_csv(PATH+'job_posts_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19784935",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of jobs Posts by cities on Daily Basis\")\n",
    "jobs.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b5b40e",
   "metadata": {},
   "source": [
    "calculate average salary per job title and state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d26854",
   "metadata": {},
   "outputs": [],
   "source": [
    "sal =new_spark_df.select( 'job_title','state',split(\"salary_offered\",\"[$/-]\").getItem(1).cast('double').alias('salary'))\n",
    "sal=sal.na.drop()\n",
    "salary_df =sal.groupBy('job_title','state').agg(avg('salary').alias('salary in $/hour'),count('*').alias('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10340be3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame and Save to Local\n",
    "avg_salary = salary_df.na.drop().toPandas()\n",
    "avg_salary.to_csv(PATH+\"avg_salary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f41970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average salary on the basis of Job title and State\")\n",
    "salary_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b21f8",
   "metadata": {},
   "source": [
    "Identify the top 10 most active companies by number of positions opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c8222",
   "metadata": {},
   "outputs": [],
   "source": [
    "company=spark_df.groupBy([\"company_name\"]).agg(count(\"post_date\").alias(\"number_of_position\"))\n",
    "company =company.sort(desc(\"number_of_position\")).limit(10)\n",
    "\n",
    "# Convert to Pandas DataFrame and Save to Local\n",
    "company_pd_df=company.toPandas()\n",
    "company_pd_df.to_csv(PATH+'Top_10_companies.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40384dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 Most Active Companies\")\n",
    "company.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06399d76",
   "metadata": {},
   "source": [
    "Create a UDF Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3a4a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create a UDF function to clean job description from HTML code contained inside\n",
    "# analyze and comment on the performance of the UDF\n",
    "# function. How the performance could be improved\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "clean_html =udf(lambda row: re.sub(re.compile('<.*?>'), '',row))\n",
    "new_spark_df =new_spark_df.withColumn('html_job_description', clean_html(\"html_job_description\"))\n",
    "print(f\"Execution time: {time.time() - start_time}\")\n",
    "\n",
    "# Using split() function cleanup HTML\n",
    "df2 = new_spark_df.select(split(col(\"html_job_description\"),\"Job Description \",2)\\\n",
    "                          .getItem(1).alias(\"new_job_description\")) \\\n",
    "                           .drop(\"html_job_description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb750987",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"SparkSession stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
